import streamlit as st
st.set_page_config(
    page_title="Satya Check", 
    page_icon="üïµüèª",
    layout="wide"
)

# Load a modern Google Font (Montserrat)
st.markdown("""
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap" rel="stylesheet">
""", unsafe_allow_html=True)

import re
import requests
from bs4 import BeautifulSoup
import spacy
import praw
from googleapiclient.discovery import build
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime, timedelta
import plotly.express as px
import pandas as pd
import numpy as np
import io
import csv

# Initialize spaCy model
nlp = spacy.load("en_core_web_sm")

# API Configurations (Replace with your own API keys in production)
REDDIT_CLIENT_ID = "zUm08w-ffHWzi9jim_eq7w"
REDDIT_CLIENT_SECRET = "zjvo8yfii4iEHRLBXbC2-55gQf3LAQ"
REDDIT_USER_AGENT = "script:news_verifier_app:1.0 (by /u/Shlong_up )"
YOUTUBE_API_KEY = "AIzaSyCQpMim9b8Se2ArZpEJjRBAzK7jaDl4-Ls"

# Initialize Reddit and YouTube clients
reddit = praw.Reddit(
    client_id=REDDIT_CLIENT_ID,
    client_secret=REDDIT_CLIENT_SECRET,
    user_agent=REDDIT_USER_AGENT
)
youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)

def apply_custom_css():
    """
    Apply custom CSS to create a modern, sleek UI.
    """
    st.markdown("""
    <style>
    /* Import Google Font */
    body, button, input, textarea {
        font-family: 'Montserrat', sans-serif;
    }
    
    /* Global Styles */
    body {
        background-color: #F0F2F5;
        color: #333;
        margin: 0;
        padding: 0;
    }
    
    .main .block-container {
        padding: 2rem 1rem;
        max-width: 1200px;
        margin: 0 auto;
    }
    
    .stApp {
        background-color: #F0F2F5;
    }
    
    /* Header Styles */
    .app-title {
        font-weight: 800;
        font-size: 3rem;
        background: linear-gradient(135deg, #5B51EB, #FF6A95);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 0.5rem;
        text-shadow: 1px 1px 3px rgba(0,0,0,0.1);
    }
    
    .app-subtitle {
        text-align: center;
        color: #555;
        font-size: 1.2rem;
        margin-bottom: 2rem;
    }
    
    /* Sidebar Styles */
    .css-1cypcdb {
        background-color: #fff !important;
        border-radius: 10px;
        padding: 1.5rem;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
    }
    
    /* Card Styles */
    .result-section {
        background-color: #fff;
        border-radius: 12px;
        padding: 1.5rem;
        margin-bottom: 1.5rem;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
    }
    
    .post-card {
        background-color: #fff;
        border-radius: 12px;
        padding: 1.5rem;
        margin-bottom: 1rem;
        box-shadow: 0 2px 6px rgba(0,0,0,0.08);
        border-left: 4px solid #5B51EB;
        transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    
    .post-card:hover {
        transform: translateY(-3px);
        box-shadow: 0 8px 16px rgba(0,0,0,0.1);
    }
    
    /* Button Styling */
    .stButton > button {
        background: linear-gradient(135deg, #5B51EB, #7E76F0);
        color: #fff;
        border: none;
        border-radius: 50px;
        padding: 0.75rem 2rem;
        font-weight: 700;
        font-size: 1rem;
        transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(0,0,0,0.15);
    }
    
    /* Input Styling */
    .stTextInput > div > div > input, .stTextArea > div > div > textarea {
        border-radius: 8px;
        border: 1px solid #ccc;
        padding: 1rem;
        font-size: 1rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        transition: border-color 0.3s ease, box-shadow 0.3s ease;
    }
    
    .stTextInput > div > div > input:focus, .stTextArea > div > div > textarea:focus {
        border-color: #5B51EB;
        box-shadow: 0 0 0 3px rgba(91, 81, 235, 0.15);
    }
    
    /* Custom badge for credibility */
    .credibility-badge {
        display: inline-block;
        padding: 0.5rem 1rem;
        border-radius: 50px;
        font-weight: 700;
        text-align: center;
        margin-bottom: 1rem;
    }
    
    .credibility-badge.reliable {
        background: linear-gradient(135deg, #34D399, #10B981);
        color: #fff;
    }
    
    .credibility-badge.uncertain {
        background: linear-gradient(135deg, #F59E0B, #D97706);
        color: #fff;
    }
    
    .credibility-badge.unreliable {
        background: linear-gradient(135deg, #EF4444, #DC2626);
        color: #fff;
    }
    
    /* Keyword Badge */
    .keyword-badge {
        display: inline-block;
        background-color: rgba(91, 81, 235, 0.1);
        color: #5B51EB;
        border-radius: 50px;
        padding: 0.3rem 0.8rem;
        margin: 0.2rem;
        font-size: 0.9rem;
        font-weight: 600;
        transition: background-color 0.3s ease, transform 0.3s ease;
    }
    
    .keyword-badge:hover {
        background-color: rgba(91, 81, 235, 0.2);
        transform: translateY(-2px);
    }
    </style>
    """, unsafe_allow_html=True)

def add_sidebar_features():
    """
    Add sidebar with additional features and settings.
    """
    st.sidebar.markdown("<div style='text-align: center;'><h2>‚öôÔ∏è Analysis Settings</h2></div>", unsafe_allow_html=True)
    st.sidebar.markdown("---")
    
    st.sidebar.markdown("### üïí Time Range")
    hours_back = st.sidebar.slider(
        "Search posts from last (hours)", 
        min_value=12, 
        max_value=240, 
        value=120, 
        step=12
    )
    
    st.sidebar.markdown("### üéØ Sensitivity")
    keyword_sensitivity = st.sidebar.slider(
        "Keyword match threshold", 
        min_value=10, 
        max_value=50, 
        value=20, 
        step=5
    )
    
    st.sidebar.markdown("### üîç Data Sources")
    platforms = st.sidebar.multiselect(
        "Select platforms to search",
        ["Reddit", "YouTube"],
        default=["Reddit", "YouTube"]
    )
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### üìä Visualization Settings")
    chart_type = st.sidebar.selectbox(
        "Primary chart type",
        ["Bar Chart", "Pie Chart", "Line Chart", "Heat Map"],
        index=0
    )
    
    show_detailed_stats = st.sidebar.checkbox("Show detailed statistics", value=True)
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("""
    <div style="background-color: #E8EBF3; padding: 1rem; border-radius: 10px; margin-top: 1rem;">
        <h4 style="color: #5B51EB; margin-top: 0;">üí° How It Works</h4>
        <p style="font-size: 0.9rem; margin-bottom: 0;">
            SatyaCheck cross-references news content with social media to assess credibility.
            Higher scores indicate stronger verification.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    return {
        'hours_back': hours_back, 
        'keyword_sensitivity': keyword_sensitivity,
        'platforms': platforms,
        'chart_type': chart_type,
        'show_detailed_stats': show_detailed_stats
    }

def extract_comprehensive_keywords(article_text: str) -> dict:
    """
    Enhanced keyword extraction:
      - Extracts noun chunks, named entities, and significant tokens.
      - Uses lowercasing and limits to top 10 keywords.
    """
    doc = nlp(article_text)
    keywords = {}
    # Use noun chunks with a basic frequency weighting
    for chunk in doc.noun_chunks:
        keyword = chunk.text.strip().lower()
        if 1 <= len(keyword.split()) <= 4:
            keywords[keyword] = keywords.get(keyword, 0) + 2
    # Add named entities with extra weight
    for ent in doc.ents:
        if ent.label_ in ['PERSON', 'ORG', 'GPE', 'EVENT']:
            keyword = ent.text.strip().lower()
            keywords[keyword] = keywords.get(keyword, 0) + 3
    # Consider individual important tokens
    for token in doc:
        if token.pos_ in ['NOUN', 'PROPN', 'VERB'] and len(token.text) > 3:
            keyword = token.lemma_.lower()
            keywords[keyword] = keywords.get(keyword, 0) + 1
    # Limit to top 10 keywords based on weight
    sorted_keywords = dict(sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:10])
    return sorted_keywords

def store_extracted_keywords(keywords: dict):
    """
    Generate CSV content for extracted keywords only.
    """
    csv_lines = ["keyword,weight"]
    for kw, wt in keywords.items():
        csv_lines.append(f"{kw},{wt}")
    return "\n".join(csv_lines)

def generate_full_csv(input_text, keywords, matched_posts):
    """
    Generate a CSV file with the original input text, extracted keywords,
    and details for each matched social media post.
    """
    output = io.StringIO()
    writer = csv.writer(output)
    # Write header row
    writer.writerow(["Input Text", "Extracted Keywords", "Matched Site URL", "Matched Site Title", "Platform", "Keyword Match Percentage"])
    # Format extracted keywords as "keyword: weight" pairs separated by semicolon
    kw_str = "; ".join([f"{kw}: {wt}" for kw, wt in keywords.items()])
    if matched_posts:
        for post in matched_posts:
            writer.writerow([input_text, kw_str, post.get("url", ""), post.get("title", ""), post.get("platform", ""), post.get("keyword_match_percentage", "")])
    else:
        writer.writerow([input_text, kw_str, "", "", "", ""])
    return output.getvalue()

def calculate_keyword_match(post_text: str, original_keywords: dict) -> float:
    """
    Calculate the percentage of keyword weight matched in a post using regex with word boundaries.
    """
    post_text = post_text.lower()
    total_weight = sum(original_keywords.values())
    matched_weight = 0
    matched_keywords = {}
    for keyword, weight in original_keywords.items():
        pattern = r'\b' + re.escape(keyword) + r'\b'
        if re.search(pattern, post_text):
            matched_weight += weight
            matched_keywords[keyword] = weight
    match_percentage = (matched_weight / total_weight) * 100 if total_weight else 0
    return round(match_percentage, 2), matched_keywords

def calculate_text_similarity(article_text: str, post_text: str) -> float:
    """
    Calculate cosine similarity between article text and a post's text.
    Returns similarity as a percentage.
    """
    try:
        vectorizer = TfidfVectorizer()
        tfidf = vectorizer.fit_transform([article_text, post_text])
        similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]
        return round(similarity * 100, 2)
    except Exception as e:
        st.error(f"Error calculating similarity: {e}")
        return 0.0

def assess_news_credibility(posts: list) -> dict:
    """
    Assess news credibility based on the highest keyword match percentages.
    """
    if not posts:
        return {
            'credibility_score': 0,
            'max_keyword_match': 0,
            'avg_keyword_match': 0,
            'credibility_status': 'No Matching Posts',
            'status_description': 'Insufficient data to verify the news.',
            'color': 'gray',
            'class': 'uncertain'
        }
    
    match_percents = [post['keyword_match_percentage'] for post in posts]
    max_match = max(match_percents)
    avg_match = sum(match_percents) / len(match_percents)
    
    if max_match >= 70:
        return {
            'credibility_score': max_match,
            'max_keyword_match': max_match,
            'avg_keyword_match': round(avg_match, 2),
            'credibility_status': 'Reliable News',
            'status_description': 'The news is well-supported by social media.',
            'color': '#10B981', 
            'class': 'reliable'
        }
    elif max_match >= 50:
        return {
            'credibility_score': max_match,
            'max_keyword_match': max_match,
            'avg_keyword_match': round(avg_match, 2),
            'credibility_status': 'Uncertain News',
            'status_description': 'Limited social media corroboration. Further investigation recommended.',
            'color': '#F59E0B',
            'class': 'uncertain'
        }
    else:
        return {
            'credibility_score': max_match,
            'max_keyword_match': max_match,
            'avg_keyword_match': round(avg_match, 2),
            'credibility_status': 'Likely Untrue',
            'status_description': 'Minimal social media support detected.',
            'color': '#EF4444',
            'class': 'unreliable'
        }

def override_credibility_by_post_count(posts: list) -> dict:
    """
    Override credibility if less than 5 relevant posts are found.
    """
    if len(posts) < 5:
        return {
            'credibility_score': 0,
            'max_keyword_match': 0,
            'avg_keyword_match': 0,
            'credibility_status': 'Not Reliable News',
            'status_description': 'Less than 5 supporting posts found.',
            'color': '#EF4444',
            'class': 'unreliable'
        }
    return None

def fetch_comprehensive_posts(keywords: dict, platforms: list, max_posts: int = 100, hours_back: int = 120) -> list:
    """
    Fetch posts from Reddit and YouTube using the extracted keywords.
    The query is now formed with "OR" between keywords for better recall.
    """
    posts = []
    matched_keywords_data = {}
    platform_counts = {"Reddit": 0, "YouTube": 0}
    
    try:
        time_threshold = datetime.utcnow() - timedelta(hours=hours_back)
        query = " OR ".join(keywords.keys())
        
        if "Reddit" in platforms:
            for submission in reddit.subreddit('all').search(query, limit=max_posts, sort='new'):
                post_time = datetime.fromtimestamp(submission.created_utc)
                if post_time > time_threshold:
                    full_text = f"{submission.title} {submission.selftext}"
                    keyword_match, matched_keywords = calculate_keyword_match(full_text, keywords)
                    for kw, wt in matched_keywords.items():
                        matched_keywords_data[kw] = matched_keywords_data.get(kw, 0) + 1
                    if keyword_match > 0:
                        post_info = {
                            'platform': 'Reddit',
                            'title': submission.title,
                            'text': submission.selftext,
                            'score': submission.score,
                            'num_comments': submission.num_comments,
                            'url': submission.url,
                            'timestamp': post_time.strftime("%Y-%m-%d %H:%M:%S"),
                            'keyword_match_percentage': keyword_match,
                            'matched_keywords': matched_keywords
                        }
                        posts.append(post_info)
                        platform_counts["Reddit"] += 1
        
        if "YouTube" in platforms:
            search_response = youtube.search().list(
                q=query,
                part="id,snippet",
                maxResults=max_posts,
                type="video",
                order="date"
            ).execute()
            for item in search_response.get("items", []):
                video_id = item["id"]["videoId"]
                video_response = youtube.videos().list(
                    part="statistics,snippet",
                    id=video_id
                ).execute()
                if video_response.get("items"):
                    video = video_response["items"][0]
                    published_time = datetime.strptime(video['snippet']['publishedAt'], "%Y-%m-%dT%H:%M:%SZ")
                    if published_time > time_threshold:
                        full_text = f"{video['snippet']['title']} {video['snippet']['description']}"
                        keyword_match, matched_keywords = calculate_keyword_match(full_text, keywords)
                        for kw, wt in matched_keywords.items():
                            matched_keywords_data[kw] = matched_keywords_data.get(kw, 0) + 1
                        if keyword_match > 0:
                            post_info = {
                                'platform': 'YouTube',
                                'title': video['snippet']['title'],
                                'description': video['snippet']['description'],
                                'views': int(video['statistics'].get('viewCount', 0)),
                                'likes': int(video['statistics'].get('likeCount', 0)),
                                'url': f"https://youtu.be/{video_id}",
                                'timestamp': published_time.strftime("%Y-%m-%d %H:%M:%S"),
                                'keyword_match_percentage': keyword_match,
                                'matched_keywords': matched_keywords
                            }
                            posts.append(post_info)
                            platform_counts["YouTube"] += 1
        
        posts = sorted(posts, key=lambda p: p['keyword_match_percentage'], reverse=True)
        return posts, matched_keywords_data, platform_counts
    except Exception as e:
        st.error(f"Error fetching posts: {e}")
        return [], {}, {"Reddit": 0, "YouTube": 0}

def display_big_credibility_status(credibility: dict):
    """
    Display the credibility status in an eye-catching design.
    """
    st.markdown(f"""
    <div style="display: flex; justify-content: center; align-items: center; margin: 2rem 0;">
        <div class="credibility-badge {credibility['class']}" style="font-size: 1.8rem; padding: 1rem 2rem;">
            {credibility['credibility_status']}
        </div>
    </div>
    """, unsafe_allow_html=True)

def create_statistics_visualizations(posts, keywords_data, platform_counts, chart_type="Bar Chart"):
    """
    Create statistical visualizations based on posts and keywords data.
    """
    if not posts:
        st.warning("No data available for visualization.")
        return
    
    st.markdown("""
    <div class="result-section">
        <h3>üìä Statistical Analysis</h3>
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        platform_df = pd.DataFrame({
            'Platform': ['Reddit', 'YouTube'],
            'Posts': [platform_counts['Reddit'], platform_counts['YouTube']]
        })
        if chart_type == "Pie Chart":
            fig = px.pie(platform_df, values='Posts', names='Platform', title='Platform Distribution', 
                         color='Platform', color_discrete_map={'Reddit': '#FF4500', 'YouTube': '#FF0000'}, hole=0.4)
            fig.update_layout(margin=dict(t=40, b=20, l=20, r=20), height=300)
            st.plotly_chart(fig, use_container_width=True)
        else:
            fig = px.bar(platform_df, x='Platform', y='Posts', title='Platform Distribution', 
                         color='Platform', color_discrete_map={'Reddit': '#FF4500', 'YouTube': '#FF0000'}, text='Posts')
            fig.update_layout(xaxis_title=None, yaxis_title="Number of Posts", showlegend=False, margin=dict(t=40, b=20, l=20, r=20), height=300)
            fig.update_traces(textposition='auto')
            st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        match_ranges = ["0-25%", "26-50%", "51-75%", "76-100%"]
        match_counts = [
            len([p for p in posts if p['keyword_match_percentage'] < 25]),
            len([p for p in posts if 25 <= p['keyword_match_percentage'] < 50]),
            len([p for p in posts if 50 <= p['keyword_match_percentage'] < 75]),
            len([p for p in posts if p['keyword_match_percentage'] >= 75])
        ]
        match_df = pd.DataFrame({
            'Match Range': match_ranges,
            'Count': match_counts
        })
        if chart_type == "Pie Chart":
            fig = px.pie(match_df, values='Count', names='Match Range', title='Keyword Match Distribution',
                         color='Match Range', color_discrete_map={'0-25%': '#EF4444', '26-50%': '#F59E0B', '51-75%': '#3B82F6', '76-100%': '#10B981'}, hole=0.4)
            fig.update_layout(margin=dict(t=40, b=20, l=20, r=20), height=300)
            st.plotly_chart(fig, use_container_width=True)
        else:
            fig = px.bar(match_df, x='Match Range', y='Count', title='Keyword Match Distribution',
                         color='Match Range', color_discrete_map={'0-25%': '#EF4444', '26-50%': '#F59E0B', '51-75%': '#3B82F6', '76-100%': '#10B981'}, text='Count')
            fig.update_layout(xaxis_title=None, yaxis_title="Number of Posts", showlegend=False, margin=dict(t=40, b=20, l=20, r=20), height=300)
            fig.update_traces(textposition='auto')
            st.plotly_chart(fig, use_container_width=True)
    
    with col3:
        dates = [datetime.strptime(p['timestamp'], "%Y-%m-%d %H:%M:%S").date() for p in posts]
        date_counts = {}
        for date in dates:
            date_str = date.strftime("%Y-%m-%d")
            date_counts[date_str] = date_counts.get(date_str, 0) + 1
        time_df = pd.DataFrame({
            'Date': list(date_counts.keys()),
            'Posts': list(date_counts.values())
        })
        time_df['Date'] = pd.to_datetime(time_df['Date'])
        time_df = time_df.sort_values('Date')
        if chart_type in ["Line Chart", "Heat Map"]:
            fig = px.line(time_df, x='Date', y='Posts', title='Posts Over Time', markers=True)
            fig.update_layout(xaxis_title=None, yaxis_title="Number of Posts", margin=dict(t=40, b=20, l=20, r=20), height=300)
            st.plotly_chart(fig, use_container_width=True)
        else:
            fig = px.bar(time_df, x='Date', y='Posts', title='Posts Over Time', text='Posts')
            fig.update_layout(xaxis_title=None, yaxis_title="Number of Posts", margin=dict(t=40, b=20, l=20, r=20), height=300)
            fig.update_traces(textposition='auto')
            st.plotly_chart(fig, use_container_width=True)
    
    if keywords_data:
        st.markdown("<h4>üîë Keyword Frequency Analysis</h4>", unsafe_allow_html=True)
        keyword_df = pd.DataFrame({
            'Keyword': list(keywords_data.keys()),
            'Frequency': list(keywords_data.values())
        }).sort_values('Frequency', ascending=False).head(10)
        if chart_type == "Bar Chart":
            fig = px.bar(keyword_df, x='Keyword', y='Frequency', color='Frequency', color_continuous_scale='Blues',
                         title='Top Keywords in Posts', text='Frequency')
            fig.update_layout(xaxis_title=None, yaxis_title="Frequency", margin=dict(t=40, b=20, l=20, r=20), xaxis={'categoryorder':'total descending'})
            fig.update_traces(textposition='auto')
            st.plotly_chart(fig, use_container_width=True)
        elif chart_type == "Heat Map":
            keyword_matrix = np.array(keyword_df['Frequency']).reshape(len(keyword_df), 1)
            fig = px.imshow(keyword_matrix, labels=dict(x="", y="Keyword", color="Frequency"),
                            y=keyword_df['Keyword'], color_continuous_scale='Blues', aspect="auto", title='Keyword Intensity Heatmap')
            fig.update_layout(margin=dict(t=40, b=20, l=150, r=20), height=400)
            st.plotly_chart(fig, use_container_width=True)
        else:
            fig = px.pie(keyword_df, values='Frequency', names='Keyword', title='Keyword Distribution', hole=0.4)
            fig.update_layout(margin=dict(t=40, b=20, l=20, r=20))
            st.plotly_chart(fig, use_container_width=True)
    
    if len(posts) >= 5:
        st.markdown("<h4>üéØ Keyword Match Score Distribution</h4>", unsafe_allow_html=True)
        match_scores = [p['keyword_match_percentage'] for p in posts]
        fig = px.histogram(match_scores, nbins=20, title='Distribution of Keyword Match Scores',
                           color_discrete_sequence=['#5B51EB'], marginal="box")
        fig.update_layout(xaxis_title="Keyword Match Score (%)", yaxis_title="Count", bargap=0.1, margin=dict(t=40, b=20, l=20, r=20))
        st.plotly_chart(fig, use_container_width=True)
    
def display_analyzed_posts(posts, keywords, show_details=True):
    """
    Display the top matching social media posts with detailed info.
    """
    if not posts:
        st.warning("No matching posts found.")
        return
    
    st.markdown("""
    <div class="result-section">
        <h3>üîç Analyzed Social Media Posts</h3>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown(f"<p>Showing top {min(10, len(posts))} most relevant posts:</p>", unsafe_allow_html=True)
    
    for post in posts[:10]:
        platform_class = "reddit" if post['platform'] == "Reddit" else "youtube"
        platform_icon = "üî¥" if post['platform'] == "Reddit" else "‚ñ∂Ô∏è"
        
        st.markdown(f"""
        <div class="post-card {platform_class}">
            <div style="display: flex; justify-content: space-between; margin-bottom: 0.5rem;">
                <span style="font-weight: 700;">{platform_icon} {post['platform']}</span>
                <span style="background: linear-gradient(135deg, {get_gradient_colors(post['keyword_match_percentage'])});
                      color: #fff; padding: 0.25rem 0.75rem; border-radius: 50px; font-weight: 700;">
                    Match: {post['keyword_match_percentage']}%
                </span>
            </div>
            <h4 style="margin: 0.5rem 0;">{post['title']}</h4>
            <div style="display: flex; gap: 1rem; margin-bottom: 0.5rem;">
                <span>üìÖ {post['timestamp']}</span>
                {format_platform_stats(post)}
            </div>
            {display_matched_keywords(post, keywords) if show_details else ''}
            <div style="margin-top: 1rem;">
                <a href="{post['url']}" target="_blank" style="color: #5B51EB; text-decoration: none; font-weight: 600;">
                    View Original Post ‚Üí
                </a>
            </div>
        </div>
        """, unsafe_allow_html=True)

def get_gradient_colors(match_percentage):
    if match_percentage >= 75:
        return "#10B981 0%, #059669 100%"
    elif match_percentage >= 50:
        return "#3B82F6 0%, #2563EB 100%"
    elif match_percentage >= 25:
        return "#F59E0B 0%, #D97706 100%"
    else:
        return "#EF4444 0%, #DC2626 100%"

def format_platform_stats(post):
    if post['platform'] == "Reddit":
        return f"""<span>üëç {post.get('score', 0)}</span>
                   <span>üí¨ {post.get('num_comments', 0)}</span>"""
    else:
        return f"""<span>üëÅÔ∏è {format_number(post.get('views', 0))}</span>
                   <span>üëç {format_number(post.get('likes', 0))}</span>"""

def format_number(num):
    if num >= 1_000_000_000:
        return f"{num / 1_000_000_000:.1f}B"
    elif num >= 1_000_000:
        return f"{num / 1_000_000:.1f}M"
    elif num >= 1_000:
        return f"{num / 1_000:.1f}K"
    return str(num)

def display_matched_keywords(post, original_keywords):
    if not post.get('matched_keywords'):
        return ""
    
    matched_html = "<div style='margin-top: 0.75rem;'><strong>Matched Keywords:</strong> "
    for keyword, weight in post['matched_keywords'].items():
        importance = min(100, int((weight / max(original_keywords.values())) * 100))
        opacity = 0.5 + (importance / 200)
        matched_html += f"""<span class="keyword-badge" style="opacity: {opacity};">{keyword}</span>"""
    matched_html += "</div>"
    return matched_html

def display_sentiment_analysis(posts):
    if not posts:
        return
    
    st.markdown("""
    <div class="result-section">
        <h3>üòä Sentiment Analysis</h3>
    </div>
    """, unsafe_allow_html=True)
    
    sentiments = {"Positive": 0, "Neutral": 0, "Negative": 0}
    for post in posts:
        match = post['keyword_match_percentage']
        if match >= 70:
            sentiments["Positive"] += 1
        elif match >= 40:
            sentiments["Neutral"] += 1
        else:
            sentiments["Negative"] += 1
    
    sentiment_df = pd.DataFrame({
        'Sentiment': list(sentiments.keys()),
        'Count': list(sentiments.values())
    })
    
    fig = px.pie(sentiment_df, values='Count', names='Sentiment', title='Sentiment Distribution',
                 color='Sentiment', color_discrete_map={'Positive': '#10B981', 'Neutral': '#3B82F6', 'Negative': '#EF4444'}, hole=0.4)
    fig.update_layout(margin=dict(t=40, b=20, l=20, r=20))
    st.plotly_chart(fig, use_container_width=True)

def display_keywords_analysis(keywords):
    if not keywords:
        return
    
    st.markdown("""
    <div class="result-section">
        <h3>üîë Extracted Keywords</h3>
        <p>These keywords were extracted from the news article for verification:</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("<div style='display: flex; flex-wrap: wrap; gap: 0.5rem; margin-top: 1rem;'>", unsafe_allow_html=True)
    max_weight = max(keywords.values())
    for keyword, weight in keywords.items():
        importance = int((weight / max_weight) * 100)
        font_size = 0.9 + (importance / 100)
        opacity = 0.6 + (importance / 250)
        st.markdown(f"""
        <div style="
            background-color: rgba(91, 81, 235, 0.1);
            color: #5B51EB;
            border-radius: 50px;
            padding: 0.5rem 1rem;
            font-size: {font_size}rem;
            font-weight: {600 + (importance // 20)};
            opacity: {opacity};
            transition: all 0.3s ease;
        ">
            {keyword}
        </div>
        """, unsafe_allow_html=True)
    st.markdown("</div>", unsafe_allow_html=True)

def main():
    apply_custom_css()
    settings = add_sidebar_features()
    
    st.markdown("<div class='app-title'>SatyaCheck</div>", unsafe_allow_html=True)
    st.markdown("<div class='app-subtitle'>Verify news credibility through social media cross-referencing</div>", unsafe_allow_html=True)
    
    tabs = st.tabs(["Text Analysis", "About"])
    
    with tabs[0]:
        st.markdown("### üìù Paste news article text to verify")
        article_text = st.text_area("News article text", height=300, placeholder="Paste the complete news article text here...")
        
        analyze_text_button = st.button("Analyze Text")
        
        if analyze_text_button and article_text:
            with st.spinner("Extracting keywords..."):
                keywords = extract_comprehensive_keywords(article_text)
                display_keywords_analysis(keywords)
                st.session_state["extracted_keywords"] = keywords
            with st.spinner("Fetching social media posts..."):
                posts, matched_keywords_data, platform_counts = fetch_comprehensive_posts(
                    keywords, 
                    settings['platforms'],
                    hours_back=settings['hours_back']
                )
                filtered_posts = [p for p in posts if p['keyword_match_percentage'] >= settings['keyword_sensitivity']]
                st.success(f"Found {len(filtered_posts)} relevant posts (matched ‚â•{settings['keyword_sensitivity']}% keywords)")
                credibility = assess_news_credibility(filtered_posts)
                override = override_credibility_by_post_count(filtered_posts) if len(filtered_posts) < 5 else None
                if override:
                    credibility = override
                display_big_credibility_status(credibility)
                create_statistics_visualizations(filtered_posts, matched_keywords_data, platform_counts, chart_type=settings['chart_type'])
                display_sentiment_analysis(filtered_posts)
                display_analyzed_posts(filtered_posts, keywords, settings['show_detailed_stats'])
            # Generate a full CSV with input text, extracted keywords and matched post results
            full_csv = generate_full_csv(article_text, keywords, filtered_posts)
            st.download_button("Download Full Results as CSV", full_csv, "full_results.csv", "text/csv")
    
    with tabs[1]:
        st.markdown("""
        <div class="result-section">
            <h3>‚ÑπÔ∏è About SatyaCheck</h3>
            <p>SatyaCheck is a powerful tool designed to verify news credibility by cross-referencing social media. The name "Satya" means "truth" in Sanskrit, reflecting our commitment to reliable information.</p>
            <h4>How It Works</h4>
            <ol>
                <li>Paste the news article text.</li>
                <li>SatyaCheck extracts key information and identifies important keywords.</li>
                <li>The system searches social media platforms for related content.</li>
                <li>Advanced algorithms analyze social validation.</li>
                <li>You receive a credibility assessment based on the findings.</li>
            </ol>
            <h4>Why SatyaCheck?</h4>
            <p>In today‚Äôs fast-paced media environment, distinguishing fact from fiction is challenging. SatyaCheck leverages collective social insights to help you identify reliable news.</p>
            <p><strong>Note:</strong> This tool should be one of many resources used for media literacy. Always verify information with multiple sources.</p>
        </div>
        """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
